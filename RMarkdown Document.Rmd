---
title: "Data Jobs in the U.S. Government"
author: "Andrew Cavalier"
date: "11/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview

Project is on [Github](https://github.com/Adrewcav/API-Data-Collection-)

The goal of this project is to answer the business question: How to become a Data Analyst in the U.S. government?
Therefore, this document showcases the requirements for data-related job fields with the U.S. Government. The data used in this project is collated from the USAJobs.gov API. **The project is broken down into the following steps**

### Step 1: Extract Data
For this project, I extracted live data (or Requested it) using API calls looking for **key search terms** with the USAJobs.gov Rest API. This process was useful for collecting large amounts of live data from a reliable, publicly available source. The drawback with this data however, is that it needs to be cleaned. 

Key Search Terms: 

* Scientist
* Data
* Analyst
* Data Scientist 

The results of the data extraction with these key words is the creation of 4 separate data sets that will be cleaned for analysis later on. 

**Note**, it is common for agencies to define data job duties and their job titles differently even if they are technically the same job per the Job Description (i.e. "Computer Scientist" and "Data Scientist"). The different naming conventions sometimes reflect the specialization or scope of the position within the Data Science field, however they also frequently do not. Therefore, assumptions are made that the data may not reflect all position titles or currently available positions that the U.S. government has on offer for the Data Science Field. 

### Step 2: Extract Data frame to Excel File

A **write_xlsx** command was utilized to extract the data frame for each data set into excel files. The Excel files hold the data from each data frame made with an API call and are represented by the names **Scientist**, **Data**, **Analyst**, and **Data Scientist**.

The data is stored.. 

### Step 3: Automate Entire Process

* Run Cron Job to run script automatically at set time for 1 week 

### Step 4: Clean Data
* Automate SQL and Data cleaning functions. 

### Step 5: Visualize Data 
* Take data and create visuals. Automate if possible. 

### Step 6: Insights Overview
**Key Insight Metrics:**

* Position Title 
* Organization/Agency 
* Telework Eligible 
* Grade levels
* Qualifications Summary
* Education
* Total Openings


When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.



