---
title: "Data Jobs in the U.S. Government"
author: "Andrew Cavalier"
date: "11/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview

Project is on **[Github](https://github.com/Adrewcav/API-Data-Collection-)!**

The goal of this project is to answer the business question: How to become a Data Analyst in the U.S. government?
Therefore, this document showcases the key position requirements and metrics for data-related jobs with the U.S. Government. The data used in this project is collated from the **[USAJobs.gov API](https://developer.usajobs.gov/)** and **[Office of Personnel Management (OPM)](https://www.opm.gov/policy-data-oversight/pay-leave/salaries-wages/2021/general-schedule/)** website. **The project is broken down into the following steps**

### Step 1: Extract Data from API
For this project, live data was extracted (or Requested) using API calls looking for **key search terms** with the USAJobs.gov Rest API. This process was useful for collecting large amounts of live data from a reliable, publicly available source. The drawback with this data however, is that it needs to be cleaned.

Key Search Terms: 

* Scientist
* Data
* Analyst
* Data Scientist 

The results of the data extraction with these key words is the creation of 4 separate data sets that will be cleaned for analysis later on. 

**Note**: It is common for agencies to define data job duties and their job titles differently even if they are technically the same job per the Job Description (i.e. "Computer Scientist" and "Data Scientist"). The different naming conventions sometimes reflect the specialization or scope of the position within the Data Science field, however they also frequently do not. Alongside this, data was only collected for one day (23Nov2021) and represents a snapshot in time.  Therefore, assumptions are made that the data may not reflect all position titles or currently available positions that the U.S. government has on offer for the Data Science Field. 

### Step 2: Extract Data to Excel File

A **write_xlsx** command was utilized to extract the data for each data set into excel files. The Excel files hold the data from each data set made with an API call and are represented by the names **Scientist**, **Data**, **Analyst**, and **Data Scientist**.

The raw data is combined into an excel spreadsheet and is stored **[here](https://github.com/Adrewcav/USAJobs-Data-Analysis-/blob/main/Combined%20Data%20Raw%20.xlsx)**. 


### Step 3: Clean Data
Clean data in Excel:

* **1.** All data was copied from seperate Excel files into a single Excel file, with different sheet names for the corresponding data. 

* **2.** Before cleaning, I tried to make sense of the data and determine which data is relevant to the question.

* **3.** I created a new sheet titled **"Combined"** to house the relevant data from the other sheets for cleaning and analysis.

* **4** Label relevant column titles in the new sheet (i.e. "MatchedObjectDescriptor.PositionTitle" -> "Position Title") and integrate the corresponding data.

* **5.** Remove Duplicate data. Select all data (cmd+A) -> Data -> Remove Duplicates -> select only Column A.

* **6.** Define values for certain data points.
Bachelor's, Master's, Phd, or N/A, are the values used for the data point **Minimum Degree Level**. The other data points, **Python**, **R**, **SQL**, **Tableau**, **Data Aggregation**, **Data Analysis**, and **Data Visualizations** have the binary variables, TRUE or FALSE. 

* **7.** Text mining in R.
I needed to familiarize myself with text mining, so I picked up a copy of the book "Text Mining in Practice" by Ted Kwartler to learn the basics of Keyword scanning. After reading some entries on Keyword scanning, I created a script called **"Keyword Scanner"** to review the PD column text and return results for each row. Here I utilized the **grep( )** command to search for keywords in the text and identify their columns based on the presence of the keywords. I searched for the key terms Python, R, SQL, Tableau, Data Aggregation, Data Visualization, Programming, Bachelor, Master, and Ph.D in the text. 

* **8.** Remove irrelevant jobs.
After successfully adding the entries from the keyword scanner, I did a quick review of the job titles and removed any positions that appeared irrelevant (positions without "data" in the title and with no results with the keyword scanner). In this respect, I used the results of the keyword scanner to vet "analyst" positions, as these took up a large bulk of the data and while having a probability of being a "data job" also likely could be very different. 

* **9.** Positions titles were reviewed and then renamed for ease of analysis (i.e. FINANCIAL MANAGEMENT ANALYST (DATA ANALYTICS) -> Data Analyst, etc. )

**Note**: Step 6-7 presented a challenge, since data such as minimum degree level or key skills for a position are not listed in binary terms or have a defined value. This data is present in 1 column (PD) with a 1-300 word description per row item. Therefore, before I could set a single value for each column and row, I needed to perform a separate analysis i.e. keyword scanning via Text Mining. Alongside this, some irrelevant jobs may still be present after step 8 and the final analysis may include data from jobs which may not be on the spectrum of the data science field. 

### Step 4: Analyze and Visualize Data 
* Take data and create visuals. Automate if possible. 



**Qualifications Summary**

* Minimum Degree Level: Bachelor's, Master's, Ph.D, N/A
* Skills: Python, R, SQL, Tableau, Data Aggregation, Data Visualization, Programming

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

### Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

### References

